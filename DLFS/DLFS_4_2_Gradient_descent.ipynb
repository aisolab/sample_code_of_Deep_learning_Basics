{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 밑바닥부터 시작하는 딥러닝(Deep Learning from Scratch) 챕터별 예제코드 정리 \n",
    "예제코드가 기본이나 정확히 일치하지는 않습니다. 개인적으로 모든 코드들을 문서화하되 너무 기본적인 내용은 제외하거나 나름대로 더 필요하다고 생각되는 내용은 추가하였습니다. 같은 내용을 다른 방식으로 구현하는 경우도 있습니다.\n",
    "## Chapter 4. 신경망 학습\n",
    "### 4.4 Gradient descent (경사하강법)\n",
    "추후에 계산그래프를 이용한 해석적 미분(analytic gradient)을 활용할 것이나 본 예제에서는 수치미분(numerical gradient)을 활용한다. gradient descent의 hyper-parameter로 learning rate(학습률) 또는 stepsize-parameter라는 것이 있으며 적절한 값을 정해주어야 gradient descent가 잘 동작한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "# numerical gradient 함수로 어떤 특정값에서의 다변수함수의 gradient를 계산\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## gradient descent (경사하강법)\n",
    "# init_x : 초기 x 값, lr : learning rate (학습률), step_num : iteration 횟수\n",
    "def gradient_descent(f, init_x, lr = 0.01, step_num = 100):\n",
    "    x = init_x.copy()\n",
    "    x_history = []\n",
    "    for i in range(step_num):\n",
    "        x_history.append(x.copy())\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x, np.array(x_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example 1\n",
    "gradient descent의 예제로서는 아래와 같은 함수를 활용한다. 최소값은 (0,0)일 때이다.  \n",
    "\n",
    "$$f(x_0,x_1)={x_0}^2+{x_1}^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBoIkCYD/AyF6Lx6ZlVQAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fa104b4358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example1\n",
    "def function_2(x):\n",
    "        return np.sum(x**2)\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "x, x_history = gradient_descent(function_2, init_x = init_x, lr = 0.1, step_num = 100)\n",
    "\n",
    "# example1 visualize\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.67498081e+14  -1.25119902e+12]\n",
      "[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# gradient descent or gradient ascent는 hyper-parameter인 학습률을 적절히 설정하는 것이 매우 중요\n",
    "# 학습률이 너무 클 경우 (학습률이 너무커서 발산한 것을 볼 수 있다.)\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "x, x_history = gradient_descent(function_2, init_x = init_x, lr = 100, step_num = 100)\n",
    "print(x)\n",
    "\n",
    "# 학습률이 너무 작을 경우 (gradient descent로 최적점까지 찾아가지못하였다.)\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "x, x_history = gradient_descent(function_2, init_x = init_x, lr = 1e-10, step_num = 100)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example 2\n",
    "hidden layer가 없고 2차원의 input을 받아 3차원의 output을 내놓는 단순한 network를 예제로 활용한다. 3차원의 output은 각 범주에 속할 확률값이다. 해당 예제를 구현하기위해 먼저 score값을 확률로 변환해주는 softmax layer와 classification 문제이므로 cross entropy loss를 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prerequisite\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c) # overflow의 보정\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta)) # delta는 y가 0값이 될 경우에 값이 계산안되는 경우를 보정한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 경우 가중치가 3 x 2의 행렬이 될 것이므로 numerical gradient도 행렬의 꼴을 반영할 수 있도록 변형한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    for idx1 in range(x.shape[0]):\n",
    "        for idx2 in range(x.shape[1]):\n",
    "            tmp_val = x[idx1, idx2]\n",
    "            x[idx1, idx2] = tmp_val + h\n",
    "            fxh1 = f(x)\n",
    "\n",
    "            x[idx1, idx2] = tmp_val - h\n",
    "            fxh2 = f(x)\n",
    "\n",
    "            grad[idx1, idx2] = (fxh1 - fxh2) / (2 * h)\n",
    "            x[idx1, idx2] = tmp_val\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 예제를 생성하는 class를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(3,2)\n",
    "\n",
    "    def predict(self, x):\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(2,1)\n",
    "            return softmax(np.dot(self.W, x))\n",
    "        else:\n",
    "            return softmax(np.dot(self.W, x))\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        if t.ndim == 1:\n",
    "            t = t.reshape(3,1)\n",
    "            loss = cross_entropy_error(y, t)\n",
    "            return loss\n",
    "        else:\n",
    "            loss = cross_entropy_error(y, t)\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드에서 np.array의 index 기준 1의 확률값이 높게 나왔으므로 true label의 설정에 따라 loss가 알맞게 계산됨을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1855394 ]\n",
      " [ 0.5950234 ]\n",
      " [ 0.21943721]] 1.23037527595\n",
      "[[ 0.1855394 ]\n",
      " [ 0.5950234 ]\n",
      " [ 0.21943721]] 0.854789193123\n"
     ]
    }
   ],
   "source": [
    "# forward가 올바른 결과인지 확인하기\n",
    "net = simpleNet()\n",
    "x = np.array([.6, .9])\n",
    "print(net.predict(x), net.loss(x, t = np.array([0,0,1])))\n",
    "print(net.predict(x), net.loss(x, t = np.array([0,1,0]))) # true label로 예측할 확률이 가장 컸을 경우 loss가 작음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 경우 예제에서 true label을 (0,1,0)으로 했을 때 위의 경우와 같은 x에 대해서 gradient를 아래의 코드로 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05653467,  0.084802  ],\n",
       "       [-0.12468032, -0.18702048],\n",
       "       [ 0.06814565,  0.10221848]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([.6, .9])\n",
    "t = np.array([0, 1, 0])\n",
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "numerical_gradient(f, net.W)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
