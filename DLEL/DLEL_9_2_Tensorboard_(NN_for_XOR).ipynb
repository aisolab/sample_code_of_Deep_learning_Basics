{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모두를 위한 머신러닝/딥러닝 강의\n",
    "김성훈 교수님의 모두를 위한 머신러닝/딥러닝 강의 중 lab 강의 코드입니다.\n",
    "## Lab9_2 Tensorboard (Neural Net for XOR)\n",
    "tf.name_scope와 tf.variable_scope의 차이는 아래의 링크를 참고, 서로 다른 parameter를 주고 cost/loss의 변화 같은 것을 체크하고 싶을 때, 동일 폴더안의 세부 폴더에 각각의 graph를 저장하고, 세부 폴더의 상위폴더를 cmd에서 tensorboard 명령어로 실행시킨다.\n",
    "\n",
    "http://stackoverflow.com/questions/35919020/whats-the-difference-of-name-scope-and-a-variable-scope-in-tensorflow/35927880\n",
    "\n",
    "* Tensorboard from tf.name_scope : tensorboard --logdir=./logs/xor_with_name_scope\n",
    "* Tensorboard from tf.variable_scope : tensorboard --logdir=./logs/xor_with_variable_scope\n",
    "* All tensorboard : tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with tf.name_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'data-01-test-score.csv',\n",
       " 'data-02-test-score.csv',\n",
       " 'data-03-test-score.csv',\n",
       " 'data-04-test-score.csv',\n",
       " 'data-05-test-score.csv',\n",
       " 'diabetes.csv',\n",
       " 'DLEL.zip',\n",
       " 'DLEL_1_TensorFlow_Basics.ipynb',\n",
       " 'DLEL_2_Linear_regression.ipynb',\n",
       " 'DLEL_3_Minimizing_Cost.ipynb',\n",
       " 'DLEL_4_1_Multi-variable_linear_regression.ipynb',\n",
       " 'DLEL_4_2_Loading_Data_from_File.ipynb',\n",
       " 'DLEL_5_Logistic_regression.ipynb',\n",
       " 'DLEL_6_Softmax_Classifier.ipynb',\n",
       " 'DLEL_7_1_Learning_rate_&_Evaluation_&_pre-processing.ipynb',\n",
       " 'DLEL_7_2_Softmax_classifier_for_mnist_data.ipynb',\n",
       " 'DLEL_8_Tensor_Manipulation.ipynb',\n",
       " 'DLEL_9_1_NN_for_XOR.ipynb',\n",
       " 'DLEL_9_2_Tensorboard_(NN_for_XOR).ipynb',\n",
       " 'logs',\n",
       " 'MNIST_data',\n",
       " 'Untitled.ipynb',\n",
       " 'zoo.csv']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, sys\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  1.]] [[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "# XOR data setting\n",
    "#tf.set_random_seed(777)  # for reproducibility\n",
    "x_data = [[0, 0],\n",
    "          [0, 1],\n",
    "          [1, 0],\n",
    "          [1, 1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "x_data = np.array(x_data, dtype = np.float32)\n",
    "y_data = np.array(y_data, dtype = np.float32)\n",
    "print(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# graph setting\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [None, 2], name = 'x-input')\n",
    "Y = tf.placeholder(dtype = tf.float32, shape = [None, 1], name = 'y-input')\n",
    "\n",
    "\n",
    "with tf.name_scope('layer1') as scope:\n",
    "    w1 = tf.Variable(initial_value = tf.random_normal(shape = [2,2]), name = 'weight1')\n",
    "    b1 = tf.Variable(initial_value = tf.random_normal(shape = [2]), name = 'bias1')\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, w1) + b1)\n",
    "\n",
    "    # from TF graph, decide which tensors you want to log\n",
    "    w1_hist = tf.summary.histogram(name = 'weight1', values = w1)\n",
    "    b1_hist = tf.summary.histogram(name = 'bias1', values = b1)\n",
    "    layer1_hist = tf.summary.histogram(name = 'layer1', values = layer1)\n",
    "    \n",
    "with tf.name_scope('layer2') as scope:\n",
    "    w2 = tf.Variable(initial_value = tf.random_normal(shape = [2,1]), name = 'weight2')\n",
    "    b2 = tf.Variable(initial_value = tf.random_normal(shape = [1]), name = 'bias2')\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, w2) + b2)\n",
    "    \n",
    "    # from TF graph, decide which tensors you want to log\n",
    "    w2_hist = tf.summary.histogram(name = 'weight2', values = w2)\n",
    "    b2_hist = tf.summary.histogram(name = 'bias2', values = b2)\n",
    "    hypothesis_hist = tf.summary.histogram(name = 'hypothesis', values = hypothesis)\n",
    "\n",
    "# cost/loss function\n",
    "with tf.name_scope('cost') as scope:\n",
    "    cost = - tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis)) \n",
    "    \n",
    "    # from TF graph, decide which tensors you want to log\n",
    "    cost_summ = tf.summary.scalar(name = 'weight', tensor = cost)\n",
    "    \n",
    "with tf.name_scope('train') as scope:\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate = 0.9).minimize(cost)\n",
    "    \n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "accuracy_summ = tf.summary.scalar(name = 'accuracy', tensor = accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.709016 [array([[ 0.45735884,  0.0843783 ],\n",
      "       [ 1.92094934, -1.26961207]], dtype=float32), array([[-1.05324161],\n",
      "       [ 0.23504813]], dtype=float32)]\n",
      "500 0.339819 [array([[-5.1176219 ,  2.42061996],\n",
      "       [ 5.48095369, -2.11626196]], dtype=float32), array([[-3.95517063],\n",
      "       [-2.84757304]], dtype=float32)]\n",
      "1000 0.0405297 [array([[-6.75340605,  5.92167854],\n",
      "       [ 6.90751934, -5.6001153 ]], dtype=float32), array([[-7.25731087],\n",
      "       [-7.36639309]], dtype=float32)]\n",
      "\n",
      "Hypothesis:  [[ 0.03581899]\n",
      " [ 0.95294172]\n",
      " [ 0.955311  ]\n",
      " [ 0.03122473]] \n",
      "Correct:  [[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/xor_with_name_scope\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/xor_with_name_scope\")\n",
    "    writer.add_graph(sess.graph)  # Show the graph\n",
    "\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1001):\n",
    "        summary, _ = sess.run([merged_summary, train], feed_dict= {X : x_data, Y : y_data})\n",
    "        writer.add_summary(summary, global_step = step)\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            print(step, sess.run(cost, feed_dict = {X : x_data, Y : y_data}), sess.run([w1, w2]))\n",
    "    \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X : x_data, Y : y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with tf.variable_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# jupyter notebook에서 multi graph를 실행할 때, 먼저 만들어놓은 graph는 default graph에 추가되는 형식이므로 이를 reset 해놓고\n",
    "# 다시 새로운 graph를 만들어서 실행하자!\n",
    "# http://stackoverflow.com/questions/35114376/error-when-computing-summaries-in-tensorflow/35117760#35117760\n",
    "tf.reset_default_graph() \n",
    "\n",
    "# graph setting\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [None, 2], name = 'x-input')\n",
    "Y = tf.placeholder(dtype = tf.float32, shape = [None, 1], name = 'y-input')\n",
    "\n",
    "\n",
    "with tf.variable_scope('layer1'):\n",
    "    w1 = tf.Variable(initial_value = tf.random_normal(shape = [2,2]), name = 'weight1')\n",
    "    b1 = tf.Variable(initial_value = tf.random_normal(shape = [2]), name = 'bias1')\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, w1) + b1)\n",
    "\n",
    "    # from TF graph, decide which tensors you want to log\n",
    "    w1_hist = tf.summary.histogram(name = 'weight1', values = w1)\n",
    "    b1_hist = tf.summary.histogram(name = 'bias1', values = b1)\n",
    "    layer1_hist = tf.summary.histogram(name = 'layer1', values = layer1)\n",
    "    \n",
    "with tf.variable_scope('layer2'):\n",
    "    w2 = tf.Variable(initial_value = tf.random_normal(shape = [2,1]), name = 'weight2')\n",
    "    b2 = tf.Variable(initial_value = tf.random_normal(shape = [1]), name = 'bias2')\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1, w2) + b2)\n",
    "    \n",
    "    # from TF graph, decide which tensors you want to log\n",
    "    w2_hist = tf.summary.histogram(name = 'weight2', values = w2)\n",
    "    b2_hist = tf.summary.histogram(name = 'bias2', values = b2)\n",
    "    hypothesis_hist = tf.summary.histogram(name = 'hypothesis', values = hypothesis)\n",
    "\n",
    "# cost/loss function\n",
    "with tf.variable_scope('cost'):\n",
    "    cost = - tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis)) \n",
    "    \n",
    "    # from TF graph, decide which tensors you want to log\n",
    "    cost_summ = tf.summary.scalar(name = 'weight', tensor = cost)\n",
    "    \n",
    "with tf.variable_scope('train'):\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate = 0.9).minimize(cost)\n",
    "    \n",
    "# Accuracy computation\n",
    "# True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype = tf.float32))\n",
    "accuracy_summ = tf.summary.scalar(name = 'accuracy', tensor = accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.714175 [array([[-1.60497534,  1.59742951],\n",
      "       [ 0.4374412 , -0.50903016]], dtype=float32), array([[ 0.49112931],\n",
      "       [-1.65160561]], dtype=float32)]\n",
      "500 0.0715504 [array([[-5.21596384,  6.09178448],\n",
      "       [ 5.48952818, -5.85669041]], dtype=float32), array([[-6.28535652],\n",
      "       [-6.23107195]], dtype=float32)]\n",
      "1000 0.0263301 [array([[-6.21959352,  6.88628292],\n",
      "       [ 6.49929762, -6.64041471]], dtype=float32), array([[-8.17014313],\n",
      "       [-8.12390232]], dtype=float32)]\n",
      "\n",
      "Hypothesis:  [[ 0.02334166]\n",
      " [ 0.97012681]\n",
      " [ 0.96968007]\n",
      " [ 0.02037392]] \n",
      "Correct:  [[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/xor_with_variable_scope\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/xor_with_variable_scope\")\n",
    "    writer.add_graph(sess.graph)  # Show the graph\n",
    "\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(1001):\n",
    "        summary, _ = sess.run([merged_summary, train], feed_dict= {X : x_data, Y : y_data})\n",
    "        writer.add_summary(summary, global_step = step)\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            print(step, sess.run(cost, feed_dict = {X : x_data, Y : y_data}), sess.run([w1, w2]))\n",
    "    \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X : x_data, Y : y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
