{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 모두를 위한 머신러닝/딥러닝 강의\n",
    "김성훈 교수님의 모두를 위한 머신러닝/딥러닝 강의 중 lab 강의 코드입니다.\n",
    "## Lab7_1 Learning rate, Evaluation, pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data (Training or Validation) and Test data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "x_data = [[1, 2, 1], [1, 3, 2], [1, 3, 4], [1, 5, 5],\n",
    "          [1, 7, 5], [1, 2, 5], [1, 6, 6], [1, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0],\n",
    "          [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1], [3, 1, 2], [3, 3, 4]]\n",
    "y_test = [[0, 0, 1], [0, 0, 1], [0, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph setting (placeholder, Variable)\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [None, 3])\n",
    "Y = tf.placeholder(dtype = tf.float32, shape = [None, 3])\n",
    "\n",
    "W = tf.Variable(initial_value = tf.random_normal(shape = [3,3]), name = 'weight')\n",
    "b = tf.Variable(initial_value = tf.random_normal(shape = [3]), name = 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph setting (hypothesis & cost)\n",
    "logits = tf.matmul(X, W) + b # pre-activation\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y))\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, axis = 1), tf.argmax(Y, axis = 1)), dtype = tf.float32))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :     0, tr_cost : 5.7320, tr_acc : 25.00%\n",
      "             val_cost : 9.8322, val_acc : 0.00%\n",
      "step :   200, tr_cost : 0.6709, tr_acc : 75.00%\n",
      "             val_cost : 0.1270, val_acc : 100.00%\n",
      "step :   400, tr_cost : 0.5270, tr_acc : 87.50%\n",
      "             val_cost : 0.0156, val_acc : 100.00%\n",
      "step :   600, tr_cost : 0.4582, tr_acc : 100.00%\n",
      "             val_cost : 0.0056, val_acc : 100.00%\n",
      "step :   800, tr_cost : 0.4116, tr_acc : 100.00%\n",
      "             val_cost : 0.0027, val_acc : 100.00%\n",
      "step :  1000, tr_cost : 0.3767, tr_acc : 100.00%\n",
      "             val_cost : 0.0014, val_acc : 100.00%\n",
      "step :  1200, tr_cost : 0.3492, tr_acc : 100.00%\n",
      "             val_cost : 0.0008, val_acc : 100.00%\n",
      "step :  1400, tr_cost : 0.3266, tr_acc : 100.00%\n",
      "             val_cost : 0.0005, val_acc : 100.00%\n",
      "step :  1600, tr_cost : 0.3077, tr_acc : 100.00%\n",
      "             val_cost : 0.0003, val_acc : 100.00%\n",
      "step :  1800, tr_cost : 0.2914, tr_acc : 100.00%\n",
      "             val_cost : 0.0002, val_acc : 100.00%\n",
      "step :  2000, tr_cost : 0.2773, tr_acc : 100.00%\n",
      "             val_cost : 0.0001, val_acc : 100.00%\n"
     ]
    }
   ],
   "source": [
    "# launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2001):\n",
    "        tr_cost, tr_acc, _ = sess.run([cost, acc, opt], feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            val_cost, val_acc = sess.run([cost, acc], feed_dict = {X : x_test, Y : y_test})\n",
    "            print('step : {:5}, tr_cost : {:.4f}, tr_acc : {:.2%}'.format(step, tr_cost, tr_acc))\n",
    "            print('             val_cost : {:.4f}, val_acc : {:.2%}'.format(val_cost, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem of gradient descent method\n",
    "#### 1. Big learning rate, small learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :     0, tr_cost : 5.7320, tr_acc : 25.00%\n",
      "             val_cost : 41.9808, val_acc : 0.00%\n",
      "step :    20, tr_cost : 83.3595, tr_acc : 50.00%\n",
      "             val_cost : 106.1966, val_acc : 0.00%\n",
      "step :    40, tr_cost : 147.0656, tr_acc : 37.50%\n",
      "             val_cost : 0.0000, val_acc : 100.00%\n",
      "step :    60, tr_cost : 68.0667, tr_acc : 62.50%\n",
      "             val_cost : 0.0000, val_acc : 100.00%\n",
      "step :    80, tr_cost : 13.1151, tr_acc : 62.50%\n",
      "             val_cost : 0.0000, val_acc : 100.00%\n",
      "step :   100, tr_cost : 102.1734, tr_acc : 50.00%\n",
      "             val_cost : 0.0000, val_acc : 100.00%\n",
      "step :   120, tr_cost : 72.9578, tr_acc : 62.50%\n",
      "             val_cost : 0.0000, val_acc : 100.00%\n",
      "step :   140, tr_cost : 41.4987, tr_acc : 62.50%\n",
      "             val_cost : 0.0000, val_acc : 100.00%\n",
      "step :   160, tr_cost : 26.8788, tr_acc : 62.50%\n",
      "             val_cost : 0.0000, val_acc : 100.00%\n",
      "step :   180, tr_cost : 72.1485, tr_acc : 62.50%\n",
      "             val_cost : 0.0000, val_acc : 100.00%\n",
      "step :   200, tr_cost : 94.3563, tr_acc : 62.50%\n",
      "             val_cost : 0.0000, val_acc : 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Big learning rate\n",
    "# graph setting (hypothesis & cost)\n",
    "logits = tf.matmul(X, W) + b # pre-activation\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y))\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, axis = 1), tf.argmax(Y, axis = 1)), dtype = tf.float32))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = 10).minimize(cost)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(201):\n",
    "        tr_cost, tr_acc, _ = sess.run([cost, acc, opt], feed_dict = {X: x_data, Y: y_data})\n",
    "        if step % 20 == 0:\n",
    "            val_cost, val_acc = sess.run([cost, acc], feed_dict = {X : x_test, Y : y_test})\n",
    "            print('step : {:5}, tr_cost : {:.4f}, tr_acc : {:.2%}'.format(step, tr_cost, tr_acc))\n",
    "            print('             val_cost : {:.4f}, val_acc : {:.2%}'.format(val_cost, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :     0, tr_cost : 5.7320, tr_acc : 25.00%\n",
      "             val_cost : 11.6447, val_acc : 0.00%\n",
      "step :    20, tr_cost : 5.7320, tr_acc : 25.00%\n",
      "             val_cost : 11.6447, val_acc : 0.00%\n",
      "step :    40, tr_cost : 5.7320, tr_acc : 25.00%\n",
      "             val_cost : 11.6447, val_acc : 0.00%\n",
      "step :    60, tr_cost : 5.7320, tr_acc : 25.00%\n",
      "             val_cost : 11.6447, val_acc : 0.00%\n",
      "step :    80, tr_cost : 5.7320, tr_acc : 25.00%\n",
      "             val_cost : 11.6447, val_acc : 0.00%\n",
      "step :   100, tr_cost : 5.7320, tr_acc : 25.00%\n",
      "             val_cost : 11.6447, val_acc : 0.00%\n",
      "step :   120, tr_cost : 5.7320, tr_acc : 25.00%\n",
      "             val_cost : 11.6447, val_acc : 0.00%\n",
      "step :   140, tr_cost : 5.7320, tr_acc : 25.00%\n",
      "             val_cost : 11.6447, val_acc : 0.00%\n",
      "step :   160, tr_cost : 5.7320, tr_acc : 25.00%\n",
      "             val_cost : 11.6447, val_acc : 0.00%\n",
      "step :   180, tr_cost : 5.7320, tr_acc : 25.00%\n",
      "             val_cost : 11.6447, val_acc : 0.00%\n",
      "step :   200, tr_cost : 5.7320, tr_acc : 25.00%\n",
      "             val_cost : 11.6447, val_acc : 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Small learning rate\n",
    "# graph setting (hypothesis & cost)\n",
    "logits = tf.matmul(X, W) + b # pre-activation\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y))\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, axis = 1), tf.argmax(Y, axis = 1)), dtype = tf.float32))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = 1e-30).minimize(cost)\n",
    "\n",
    "# launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(201):\n",
    "        tr_cost, tr_acc, _ = sess.run([cost, acc, opt], feed_dict = {X: x_data, Y: y_data})\n",
    "        #val_cost, val_acc  = sess.run([cost, acc], feed_dict = {X: x_test, Y: y_test})\n",
    "        if step % 20 == 0:\n",
    "            val_cost, val_acc = sess.run([cost, acc], feed_dict = {X : x_test, Y : y_test})\n",
    "            print('step : {:5}, tr_cost : {:.4f}, tr_acc : {:.2%}'.format(step, tr_cost, tr_acc))\n",
    "            print('             val_cost : {:.4f}, val_acc : {:.2%}'.format(val_cost, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Non-normalized inputs (example : regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4) (8, 1)\n"
     ]
    }
   ],
   "source": [
    "# Non-normalized input data\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "print(x_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph setting (placeholder, Variable)\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [None, 4])\n",
    "Y = tf.placeholder(dtype = tf.float32, shape = [None, 1])\n",
    "\n",
    "W = tf.Variable(initial_value = tf.random_normal(shape = [4, 1]), name = 'weight')\n",
    "b = tf.Variable(initial_value = tf.random_normal(shape = [1]), name = 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# graph setting (hypothesis & cost, optimizer(gd))\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.53271e+11\n",
      "200 nan\n",
      "400 nan\n",
      "600 nan\n",
      "800 nan\n",
      "1000 nan\n",
      "1200 nan\n",
      "1400 nan\n",
      "1600 nan\n",
      "1800 nan\n",
      "2000 nan\n"
     ]
    }
   ],
   "source": [
    "# launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2001):\n",
    "        cost_val, _ = sess.run([cost, opt], feed_dict = {X : x_data, Y :y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# min-mix normalized case\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, axis = 0)\n",
    "    denominator = np.max(data, axis = 0) - np.min(data, axis = 0)\n",
    "    return numerator / (denominator + 1e-10) # noise term prevents the zero division\n",
    "normalized_x_data = MinMaxScaler(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph setting (placeholder, Variable)\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [None, 4])\n",
    "Y = tf.placeholder(dtype = tf.float32, shape = [None, 1])\n",
    "\n",
    "W = tf.Variable(initial_value = tf.random_normal(shape = [4, 1]), name = 'weight')\n",
    "b = tf.Variable(initial_value = tf.random_normal(shape = [1]), name = 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graph setting (hypothesis & cost, optimizer(gd))\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 672061.0\n",
      "200 13361.2\n",
      "400 5389.76\n",
      "600 2470.84\n",
      "800 1245.34\n",
      "1000 667.174\n",
      "1200 371.819\n",
      "1400 213.753\n",
      "1600 127.004\n",
      "1800 78.7298\n",
      "2000 51.6273\n"
     ]
    }
   ],
   "source": [
    "# launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2001):\n",
    "        cost_val, _ = sess.run([cost, opt], feed_dict = {X : normalized_x_data, Y :y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
