{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모두를 위한 머신러닝/딥러닝 강의\n",
    "김성훈 교수님의 모두를 위한 머신러닝/딥러닝 강의 중 lab 강의 코드입니다.\n",
    "## Lab12_4 Stacked RNN\n",
    "Lab12_3 RNN with long sequence의 코드를 재활용하며, 단지 LSTM Cell만 아래와 같이 한층을 더 쌓는 예제\n",
    "![Alt text](http://i.imgur.com/c3L57jA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['w', 'c', ',', 'e', 'f', 'd', 'p', 'o', '.', 'a', 'b', 'h', 'g', 'm', 'n', 's', \"'\", 'y', 'k', ' ', 'i', 'u', 'l', 'r', 't']\n",
      "{'l': 22, 'w': 0, 'c': 1, 'd': 5, ',': 2, ' ': 19, 'i': 20, 'e': 3, 'f': 4, 'p': 6, 'u': 21, 'b': 10, 'o': 7, '.': 8, 'a': 9, 'g': 12, 'm': 13, 'n': 14, 'h': 11, 's': 15, \"'\": 16, 'y': 17, 't': 24, 'r': 23, 'k': 18}\n",
      "0 if you wan  -> f you want\n",
      "20  a ship, d  -> a ship, do\n",
      "40 up people   -> p people t\n",
      "60 o collect   ->  collect w\n",
      "80 on't assig  -> n't assign\n",
      "100 ks and wor  -> s and work\n",
      "120 her teach   -> er teach t\n",
      "140 ng for the  -> g for the \n",
      "160 mmensity o  -> mensity of\n",
      "[[20, 4, 19, 17, 7, 21, 19, 0, 9, 14], [4, 19, 17, 7, 21, 19, 0, 9, 14, 24], [19, 17, 7, 21, 19, 0, 9, 14, 24, 19], [17, 7, 21, 19, 0, 9, 14, 24, 19, 24], [7, 21, 19, 0, 9, 14, 24, 19, 24, 7]]\n",
      "[[4, 19, 17, 7, 21, 19, 0, 9, 14, 24], [19, 17, 7, 21, 19, 0, 9, 14, 24, 19], [17, 7, 21, 19, 0, 9, 14, 24, 19, 24], [7, 21, 19, 0, 9, 14, 24, 19, 24, 7], [21, 19, 0, 9, 14, 24, 19, 24, 7, 19]]\n"
     ]
    }
   ],
   "source": [
    "## Data pre-processing\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "\n",
    "char_set = list(set(sentence))\n",
    "char_dic = {c : i for i, c in enumerate(char_set)}\n",
    "print(char_set)\n",
    "print(char_dic)\n",
    "\n",
    "sequence_length = 10\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:(i+sequence_length)]\n",
    "    y_str = sentence[(i+1):(i+sequence_length+1)]\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(i, x_str, ' ->', y_str)\n",
    "        \n",
    "    dataX.append([char_dic.get(x) for x in x_str])\n",
    "    dataY.append([char_dic.get(y) for y in y_str])\n",
    "\n",
    "print(dataX[:5])\n",
    "print(dataY[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hyper-parameter setting\n",
    "data_dim = len(char_set)\n",
    "hidden_size = len(char_set)\n",
    "num_classes = len(char_set)\n",
    "learning_rate = 0.1\n",
    "batch_size = len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_one_hot Tensor(\"one_hot:0\", shape=(?, 10, 25), dtype=float32)\n",
      "stacked_cell_info (2-stacked) 25 (LSTMStateTuple(c=25, h=25), LSTMStateTuple(c=25, h=25))\n",
      "outputs_for_fc Tensor(\"Reshape:0\", shape=(1700, 25), dtype=float32)\n",
      "score_for_seq_loss Tensor(\"Reshape_1:0\", shape=(170, 10, 25), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## Graph setting\n",
    "X = tf.placeholder(dtype = tf.int32, shape = [None, sequence_length])\n",
    "Y = tf.placeholder(dtype = tf.int32, shape = [None, sequence_length])\n",
    "X_one_hot = tf.one_hot(indices = X, depth = num_classes)\n",
    "print('X_one_hot', X_one_hot)\n",
    "\n",
    "# Make a lstm cell with hidden_size (each unit output vector size)\n",
    "def lstm_cell():\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units = hidden_size, forget_bias = 0.9, state_is_tuple=True)\n",
    "    return cell\n",
    "\n",
    "# If before you were using: MultiRNNCell([BasicLSTMCell(...)] * num_layers),\n",
    "# change to: MultiRNNCell([BasicLSTMCell(...) for _ in range(num_layers)])\n",
    "\n",
    "stacked_cell = tf.contrib.rnn.MultiRNNCell(cells = [lstm_cell() for i in range(2)], state_is_tuple = True) # LSTM Cell을 2단으로 stack\n",
    "print('stacked_cell_info (2-stacked)', stacked_cell.output_size, stacked_cell.state_size)\n",
    "\n",
    "initial_states = stacked_cell.zero_state(batch_size = batch_size, dtype = tf.float32)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell = stacked_cell, inputs = X_one_hot, initial_state = initial_states)\n",
    "\n",
    "# FC layer\n",
    "outputs_for_fc = tf.reshape(tensor = outputs, shape = [-1, hidden_size])\n",
    "print('outputs_for_fc', outputs_for_fc)\n",
    "dense_weight = tf.get_variable(name = 'dense_weight', dtype = tf.float32,\n",
    "                              shape = [hidden_size, num_classes],\n",
    "                              initializer = tf.contrib.layers.xavier_initializer())\n",
    "bias = tf.get_variable(name = 'bias', dtype = tf.float32, shape = [num_classes], initializer = tf.random_normal_initializer())\n",
    "score = tf.matmul(outputs_for_fc, dense_weight) + bias\n",
    "\n",
    "## reshape out for sequence_loss\n",
    "score_for_seq_loss = tf.reshape(tensor = score, shape = [batch_size, sequence_length, num_classes])\n",
    "prediction = tf.argmax(input = score_for_seq_loss, axis = -1)\n",
    "print('score_for_seq_loss', score_for_seq_loss)\n",
    "seq2seq_weight = tf.ones(shape = [batch_size, sequence_length])\n",
    "seq2seq_loss = tf.contrib.seq2seq.sequence_loss(logits = score_for_seq_loss, targets = Y, weights = seq2seq_weight)\n",
    "loss = tf.reduce_mean(seq2seq_loss)\n",
    "opt = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :   0, loss : 3.5072\n",
      "ou want to -> wwwwwwwwww\n",
      "step : 300, loss : 0.2306\n",
      "less immen -> e s immens\n",
      "step : 600, loss : 0.2291\n",
      " teach the -> toach them\n",
      "step : 900, loss : 0.2290\n",
      "ther teach -> her toach \n",
      "step : 1200, loss : 0.2287\n",
      "tasks and  -> hsks and w\n",
      "step : 1500, loss : 0.2286\n",
      "them tasks ->  em tasks \n",
      "step : 1800, loss : 0.2286\n",
      "tasks and  -> hsks and w\n",
      "step : 2100, loss : 0.2286\n",
      "t rather t ->  dather te\n",
      "step : 2400, loss : 0.2286\n",
      "ach them t -> nh them to\n",
      "step : 2700, loss : 0.2287\n",
      "ensity of  -> rsity of t\n",
      "step : 2999, loss : 0.2285\n",
      "nsity of t -> dity of th\n"
     ]
    }
   ],
   "source": [
    "## train\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(3000):\n",
    "    loss_val, results, _ = sess.run(fetches = [loss, prediction, opt], feed_dict = {X : dataX, Y : dataY})\n",
    "    \n",
    "    if step % 300 == 0 or step == 2999:\n",
    "        random_number = np.random.randint(0,len(dataX))\n",
    "        print('step : {:3}, loss : {:.4f}'.format(step, loss_val))\n",
    "        print(''.join([char_set[x] for x in dataX[random_number]]), '->', ''.join([char_set[hat] for hat in results[random_number]]))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea."
     ]
    }
   ],
   "source": [
    "## Let's print the last char of each result to check it works\n",
    "for j, result in enumerate(results):\n",
    "    if j is 0:  # print all for the first result to make a sentence\n",
    "        print(''.join([char_set[t] for t in result]), end='')\n",
    "    else:\n",
    "        print(char_set[result[-1]], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"if you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Ground truth\n",
    "sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
