{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모두를 위한 머신러닝/딥러닝 강의\n",
    "김성훈 교수님의 모두를 위한 머신러닝/딥러닝 강의 중 lab 강의 코드입니다.\n",
    "## Lab10_2 Drop out, Batch normalization\n",
    "강의에서 다룬 Drop out을 mnist data에 활용해보는 코드, 네트워크는 2개의 hidden layer를 가지는 (784, 512, 256, 10)의 구조, 여기에서는 Relu activation function, He initialization을 활용하고, 기존의 stochastic gradient destenct가 아닌 Adam 방법론을 활용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\dev \n",
      " ['.ipynb_checkpoints', 'Data structures and algorithms_warm_up.ipynb', 'Data structures and algorithms_week_1.ipynb', 'DLEL', 'DLEL.zip', 'DLEL_10_1_accuracy_loss.PNG', 'DLEL_10_1_activation_histogram.PNG', 'DLEL_10_1_example_graph.PNG', 'DLEL_10_1_Weight_initialization.ipynb', 'DLEL_10_2_Drop-out_&_Batch_normalization.ipynb', 'logs', 'MNIST_data', 'py-automate', 'untitled.txt']\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777) # for reproducibility\n",
    "print(os.getcwd(),'\\n', os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "## load data and parameter\n",
    "mnist = input_data.read_data_sets('./MNIST_data/', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define He initializer\n",
    "def he_init(n_input, n_output):\n",
    "    stddev = tf.sqrt(2.0 / (n_input))\n",
    "    return tf.truncated_normal_initializer(stddev = stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# graph setting\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [None, 784], name = 'Xinput')\n",
    "Y = tf.placeholder(dtype = tf.float32, shape = [None, 10], name = 'Yinput')\n",
    "\n",
    "# dropout (keep-prob) rate 0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(dtype = tf.float32)\n",
    "\n",
    "with tf.variable_scope('layer1'):\n",
    "    w1 = tf.get_variable(name = 'w1', shape = [784, 512], initializer = he_init(784, 512))\n",
    "    b1 = tf.Variable(initial_value = tf.random_normal(shape = [512]), name = 'b1')\n",
    "    layer1 = tf.nn.relu(tf.matmul(X, w1) + b1)\n",
    "    \n",
    "with tf.variable_scope('layer2'):\n",
    "    w2 = tf.get_variable(name = 'w2', shape = [512, 256], initializer = he_init(512, 256))\n",
    "    b2 = tf.Variable(initial_value = tf.random_normal(shape = [256]), name = 'b2')\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, w2) + b2)\n",
    "    layer2 = tf.nn.dropout(x = layer2, keep_prob = keep_prob, name = 'drop-out')\n",
    "\n",
    "with tf.variable_scope('layer3'):\n",
    "    w3 = tf.get_variable(name = 'w3', shape = [256, 10], initializer = he_init(256, 10))\n",
    "    b3 = tf.Variable(initial_value = tf.random_normal(shape = [10]), name = 'b3')\n",
    "    score = tf.matmul(layer2, w3) + b3\n",
    "\n",
    "with tf.variable_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = score, labels = Y))\n",
    "    loss_scalar = tf.summary.scalar(name = 'loss', tensor = loss)\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    train = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)\n",
    "    \n",
    "with tf.variable_scope('accuracy'):\n",
    "    is_correct = tf.equal(tf.argmax(score, 1), tf.argmax(Y, axis = 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype = tf.float32))\n",
    "    accuracy_scalar = tf.summary.scalar(name = 'accuracy', tensor = accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :    0, tr_loss : 0.257, val_loss : 0.158\n",
      "epoch :    5, tr_loss : 0.015, val_loss : 0.061\n",
      "epoch :   10, tr_loss : 0.067, val_loss : 0.082\n",
      "epoch :   15, tr_loss : 0.020, val_loss : 0.083\n",
      "epoch :   20, tr_loss : 0.044, val_loss : 0.103\n",
      "epoch :   25, tr_loss : 0.001, val_loss : 0.111\n",
      "epoch :   30, tr_loss : 0.006, val_loss : 0.122\n",
      "epoch :   35, tr_loss : 0.054, val_loss : 0.128\n",
      "epoch :   40, tr_loss : 0.014, val_loss : 0.139\n",
      "epoch :   45, tr_loss : 0.026, val_loss : 0.113\n",
      "epoch :   50, tr_loss : 0.000, val_loss : 0.114\n",
      "epoch :   55, tr_loss : 0.032, val_loss : 0.115\n",
      "epoch :   60, tr_loss : 0.000, val_loss : 0.124\n",
      "epoch :   65, tr_loss : 0.001, val_loss : 0.168\n",
      "epoch :   70, tr_loss : 0.000, val_loss : 0.158\n",
      "epoch :   75, tr_loss : 0.000, val_loss : 0.185\n",
      "epoch :   80, tr_loss : 0.000, val_loss : 0.155\n",
      "epoch :   85, tr_loss : 0.000, val_loss : 0.165\n",
      "epoch :   90, tr_loss : 0.000, val_loss : 0.193\n",
      "epoch :   95, tr_loss : 0.000, val_loss : 0.201\n",
      "epoch :   99, tr_loss : 0.000, val_loss : 0.198\n",
      "0.9828\n",
      "0.9833\n"
     ]
    }
   ],
   "source": [
    "# parameter \n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('./logs/dropout')\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for step in range(total_batch):\n",
    "            \n",
    "            tr_batch_xs, tr_batch_ys = mnist.train.next_batch(batch_size = batch_size)\n",
    "            val_batch_xs, val_batch_ys = mnist.validation.next_batch(batch_size = mnist.validation.num_examples)\n",
    "            summary, _, loss_tr = sess.run([merged_summary, train, loss], feed_dict = {X : tr_batch_xs, Y : tr_batch_ys, keep_prob : 0.5})\n",
    "            loss_val = sess.run(loss, feed_dict = {X : val_batch_xs, Y : val_batch_ys, keep_prob : 1})\n",
    "                \n",
    "        if epoch % 5 == 0 or epoch == (epochs - 1):\n",
    "            print('epoch : {:4}, tr_loss : {:.3f}, val_loss : {:.3f}'.format(epoch, loss_tr, loss_val))\n",
    "        writer.add_summary(summary, global_step = epoch)\n",
    "    \n",
    "    print(sess.run(accuracy, feed_dict = {X : val_batch_xs, Y : val_batch_ys, keep_prob : 1}))\n",
    "    print(sess.run(accuracy, feed_dict = {X : mnist.test.images, Y : mnist.test.labels, keep_prob : 1}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
