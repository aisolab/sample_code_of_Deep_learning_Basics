{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모두를 위한 머신러닝/딥러닝 강의\n",
    "김성훈 교수님의 모두를 위한 머신러닝/딥러닝 강의 중 lab 강의 코드입니다.\n",
    "## Lab12_3 RNN with long sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### short sentence example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['y', 'a', 'o', 'n', ' ', 'f', 't', 'w', 'u', 'i']\n",
      "{'u': 8, 'f': 5, 'y': 0, 'a': 1, 'w': 7, 'o': 2, 't': 6, 'n': 3, 'i': 9, ' ': 4}\n"
     ]
    }
   ],
   "source": [
    "sample = ' if you want you'\n",
    "idx2char = list(set(sample)) # index -> char\n",
    "char2idx = {c : i for i, c in enumerate(idx2char)} # char -> idx\n",
    "print(idx2char)\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 9, 5, 4, 0, 2, 8, 4, 7, 1, 3, 6, 4, 0, 2, 8]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_idx = [char2idx.get(c) for c in sample] # char to index\n",
    "sample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 9, 5, 4, 0, 2, 8, 4, 7, 1, 3, 6, 4, 0, 2]]\n",
      "[[9, 5, 4, 0, 2, 8, 4, 7, 1, 3, 6, 4, 0, 2, 8]]\n"
     ]
    }
   ],
   "source": [
    "x_data = [sample_idx[:-1]] # X data sample (0 ~ n-1)\n",
    "print(x_data) # if you want yo\n",
    "y_data = [sample_idx[1:]] # Y label sample (1 ~ n)\n",
    "print(y_data) #if you want you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## hyper-parameter setting\n",
    "dic_size = len(char2idx) # RNN input size (one hot size)\n",
    "hidden_size = len(char2idx) # RNN output size\n",
    "num_classes = len(char2idx) # final output size (RNN or Softmax, etc.)\n",
    "batch_size = 1 # one sample data , one batch\n",
    "sequence_length = len(sample) - 1\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot:0\", shape=(?, 15, 10), dtype=float32)\n",
      "10 <bound method _RNNCell.zero_state of <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x000002DB8EBB0470>>\n",
      "Tensor(\"Reshape:0\", shape=(15, 10), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(1, 15, 10), dtype=float32)\n",
      "Tensor(\"ArgMax:0\", shape=(1, 15), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "## graph setting\n",
    "X = tf.placeholder(dtype = tf.int32, shape = [None, sequence_length])\n",
    "Y = tf.placeholder(dtype = tf.int32, shape = [None, sequence_length])\n",
    "X_one_hot = tf.one_hot(indices = X, depth = num_classes)\n",
    "print(X_one_hot)\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units = hidden_size, forget_bias = 0.9, state_is_tuple = True)\n",
    "print(cell.output_size, cell.zero_state)\n",
    "\n",
    "initial_state = cell.zero_state(batch_size = batch_size, dtype = tf.float32)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell = cell, inputs = X_one_hot, sequence_length = [sequence_length], initial_state = initial_state)\n",
    "\n",
    "# FC layer\n",
    "outputs_for_fc = tf.reshape(tensor = outputs, shape = [-1, hidden_size])\n",
    "print(outputs_for_fc)\n",
    "dense_weight = tf.get_variable(name = 'dense_weight', dtype = tf.float32,\n",
    "                              shape = [hidden_size, num_classes],\n",
    "                              initializer = tf.contrib.layers.xavier_initializer())\n",
    "bias = tf.get_variable(name = 'bias', dtype = tf.float32, shape = [num_classes], initializer = tf.random_normal_initializer())\n",
    "score = tf.matmul(outputs_for_fc, dense_weight) + bias\n",
    "\n",
    "## reshape out for sequence_loss\n",
    "score_for_seq_loss = tf.reshape(tensor = score, shape = [batch_size, sequence_length, num_classes])\n",
    "prediction = tf.argmax(input = score_for_seq_loss, axis = -1)\n",
    "print(score_for_seq_loss)\n",
    "print(prediction)\n",
    "seq2seq_weight = tf.ones(shape = [batch_size, sequence_length])\n",
    "seq2seq_loss = tf.contrib.seq2seq.sequence_loss(logits = score_for_seq_loss, targets = Y, weights = seq2seq_weight)\n",
    "loss = tf.reduce_mean(seq2seq_loss)\n",
    "opt = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :  0, loss : 3.0271 \n",
      "prediction wwwwwwwwwwwwwww\n",
      "step : 10, loss : 0.5021 \n",
      "prediction if you want you\n",
      "step : 20, loss : 0.0412 \n",
      "prediction if you want you\n",
      "step : 30, loss : 0.0085 \n",
      "prediction if you want you\n",
      "step : 40, loss : 0.0037 \n",
      "prediction if you want you\n",
      "step : 49, loss : 0.0025 \n",
      "prediction if you want you\n"
     ]
    }
   ],
   "source": [
    "## train\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(50):\n",
    "        loss_val, _ = sess.run(fetches = [loss, opt], feed_dict = {X : x_data, Y : y_data})\n",
    "        result = sess.run(prediction, feed_dict = {X : x_data})\n",
    "        \n",
    "        # print char using dic\n",
    "        if step % 10 == 0 or step == 49:\n",
    "            result_str = [idx2char[c] for c in np.squeeze(result)]\n",
    "            \n",
    "            print('step : {:2}, loss : {:.4f}'.format(step, loss_val), '\\nprediction', ''.join(result_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### long sentence example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'y', 'd', 'p', 'r', 'o', 'n', ' ', 'e', 'c', '.', 's', 'u', 'k', \"'\", ',', 'm', 'g', 'w', 'i', 'a', 'f', 't', 'h', 'l']\n",
      "{'k': 13, \"'\": 14, 'b': 0, 'y': 1, ',': 15, 'f': 21, 'n': 6, 'd': 2, 'm': 16, 'g': 17, 'w': 18, 'i': 19, 'e': 8, 'p': 3, 'a': 20, 'r': 4, 'o': 5, ' ': 7, '.': 10, 'c': 9, 't': 22, 'h': 23, 'l': 24, 's': 11, 'u': 12}\n",
      "0 if you wan  -> f you want\n",
      "20  a ship, d  -> a ship, do\n",
      "40 up people   -> p people t\n",
      "60 o collect   ->  collect w\n",
      "80 on't assig  -> n't assign\n",
      "100 ks and wor  -> s and work\n",
      "120 her teach   -> er teach t\n",
      "140 ng for the  -> g for the \n",
      "160 mmensity o  -> mensity of\n",
      "[[19, 21, 7, 1, 5, 12, 7, 18, 20, 6], [21, 7, 1, 5, 12, 7, 18, 20, 6, 22], [7, 1, 5, 12, 7, 18, 20, 6, 22, 7], [1, 5, 12, 7, 18, 20, 6, 22, 7, 22], [5, 12, 7, 18, 20, 6, 22, 7, 22, 5]]\n",
      "[[21, 7, 1, 5, 12, 7, 18, 20, 6, 22], [7, 1, 5, 12, 7, 18, 20, 6, 22, 7], [1, 5, 12, 7, 18, 20, 6, 22, 7, 22], [5, 12, 7, 18, 20, 6, 22, 7, 22, 5], [12, 7, 18, 20, 6, 22, 7, 22, 5, 7]]\n"
     ]
    }
   ],
   "source": [
    "## Data pre-processing\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "\n",
    "char_set = list(set(sentence))\n",
    "char_dic = {c : i for i, c in enumerate(char_set)}\n",
    "print(char_set)\n",
    "print(char_dic)\n",
    "\n",
    "sequence_length = 10\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:(i+sequence_length)]\n",
    "    y_str = sentence[(i+1):(i+sequence_length+1)]\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(i, x_str, ' ->', y_str)\n",
    "        \n",
    "    dataX.append([char_dic.get(x) for x in x_str])\n",
    "    dataY.append([char_dic.get(y) for y in y_str])\n",
    "\n",
    "print(dataX[:5])\n",
    "print(dataY[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hyper-parameter setting\n",
    "data_dim = len(char_set)\n",
    "hidden_size = len(char_set)\n",
    "num_classes = len(char_set)\n",
    "learning_rate = 0.1\n",
    "batch_size = len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot:0\", shape=(?, 10, 25), dtype=float32)\n",
      "25 <bound method _RNNCell.zero_state of <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.BasicLSTMCell object at 0x000002DB94C9FF60>>\n",
      "Tensor(\"Reshape:0\", shape=(1700, 25), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(170, 10, 25), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "## Graph setting\n",
    "X = tf.placeholder(dtype = tf.int32, shape = [None, sequence_length])\n",
    "Y = tf.placeholder(dtype = tf.int32, shape = [None, sequence_length])\n",
    "X_one_hot = tf.one_hot(indices = X, depth = num_classes)\n",
    "print(X_one_hot)\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units = hidden_size, forget_bias = 0.9, state_is_tuple = True)\n",
    "print(cell.output_size, cell.zero_state)\n",
    "\n",
    "initial_state = cell.zero_state(batch_size = batch_size, dtype = tf.float32)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell = cell, inputs = X_one_hot, initial_state = initial_state)\n",
    "\n",
    "# FC layer\n",
    "outputs_for_fc = tf.reshape(tensor = outputs, shape = [-1, hidden_size])\n",
    "print(outputs_for_fc)\n",
    "dense_weight = tf.get_variable(name = 'dense_weight', dtype = tf.float32,\n",
    "                              shape = [hidden_size, num_classes],\n",
    "                              initializer = tf.contrib.layers.xavier_initializer())\n",
    "bias = tf.get_variable(name = 'bias', dtype = tf.float32, shape = [num_classes], initializer = tf.random_normal_initializer())\n",
    "score = tf.matmul(outputs_for_fc, dense_weight) + bias\n",
    "\n",
    "## reshape out for sequence_loss\n",
    "score_for_seq_loss = tf.reshape(tensor = score, shape = [batch_size, sequence_length, num_classes])\n",
    "prediction = tf.argmax(input = score_for_seq_loss, axis = -1)\n",
    "print(score_for_seq_loss)\n",
    "seq2seq_weight = tf.ones(shape = [batch_size, sequence_length])\n",
    "seq2seq_loss = tf.contrib.seq2seq.sequence_loss(logits = score_for_seq_loss, targets = Y, weights = seq2seq_weight)\n",
    "loss = tf.reduce_mean(seq2seq_loss)\n",
    "opt = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :   0, loss : 3.4583\n",
      "of the sea -> wwwwwwwwww\n",
      "step : 300, loss : 0.2296\n",
      "r the endl ->  the endle\n",
      "step : 600, loss : 0.2289\n",
      "ople toget -> nle togeth\n",
      "step : 900, loss : 0.2287\n",
      "them tasks -> hem tasks \n",
      "step : 1200, loss : 0.2286\n",
      "m tasks an ->  tasks and\n",
      "step : 1500, loss : 0.2286\n",
      "f you want ->  you want \n",
      "step : 1800, loss : 0.2286\n",
      "'t assign  -> t assign t\n",
      "step : 2100, loss : 0.2286\n",
      "but rather -> ui rather \n",
      "step : 2400, loss : 0.2285\n",
      "ong for th ->  ' for the\n",
      "step : 2700, loss : 0.2286\n",
      " them task -> toem tasks\n",
      "step : 2999, loss : 0.2285\n",
      "ple togeth -> ,e togethe\n"
     ]
    }
   ],
   "source": [
    "## train\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(3000):\n",
    "    loss_val, results, _ = sess.run(fetches = [loss, prediction, opt], feed_dict = {X : dataX, Y : dataY})\n",
    "    \n",
    "    if step % 300 == 0 or step == 2999:\n",
    "        random_number = np.random.randint(0,len(dataX))\n",
    "        print('step : {:3}, loss : {:.4f}'.format(step, loss_val))\n",
    "        print(''.join([char_set[x] for x in dataX[random_number]]), '->', ''.join([char_set[hat] for hat in results[random_number]]))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea."
     ]
    }
   ],
   "source": [
    "## Let's print the last char of each result to check it works\n",
    "for j, result in enumerate(results):\n",
    "    if j is 0:  # print all for the first result to make a sentence\n",
    "        print(''.join([char_set[t] for t in result]), end='')\n",
    "    else:\n",
    "        print(char_set[result[-1]], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"if you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Ground truth\n",
    "sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
