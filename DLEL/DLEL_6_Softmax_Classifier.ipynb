{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모두를 위한 머신러닝/딥러닝 강의\n",
    "김성훈 교수님의 모두를 위한 머신러닝/딥러닝 강의 중 lab 강의 코드입니다.\n",
    "## Lab6 Softmax Classifier\n",
    "### Example : toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777) # for reproducibility\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype = tf.float32, shape = [None, 4])\n",
    "Y = tf.placeholder(dtype = tf.float32, shape = [None, 3])\n",
    "\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(initial_value = tf.random_normal(shape = [4, nb_classes]), name = 'weight')\n",
    "b = tf.Variable(initial_value = tf.random_normal(shape = [nb_classes]), name = 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf.nn.softmax computes softmax activations\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = - tf.reduce_mean(tf.reduce_sum(Y * tf.log(hypothesis), axis = 1))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y, axis = 1),tf.argmax(hypothesis, axis = 1)), dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.92611 0.25\n",
      "2000 0.457088 0.875\n",
      "4000 0.356639 0.875\n",
      "6000 0.304955 1.0\n",
      "8000 0.268159 1.0\n",
      "10000 0.239511 1.0\n",
      "[array([2, 2, 2, 1, 1, 1, 0, 0], dtype=int64), array([2, 2, 2, 1, 1, 1, 0, 0], dtype=int64)]\n",
      "[[  1.62854176e-02   9.83640373e-01   7.42032644e-05]] [1]\n",
      "--------------\n",
      "[[ 0.84297669  0.13680151  0.02022176]] [0]\n",
      "--------------\n",
      "[[  3.83510223e-06   3.01761646e-03   9.96978521e-01]] [2]\n",
      "--------------\n",
      "[[  1.62854176e-02   9.83640373e-01   7.42032644e-05]\n",
      " [  8.42976689e-01   1.36801511e-01   2.02217568e-02]\n",
      " [  3.83510223e-06   3.01761623e-03   9.96978521e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cost_val, _, acc_val = sess.run(fetches = [cost, opt, accuracy], feed_dict = {X : x_data, Y : y_data})\n",
    "        if step % 2000 == 0:\n",
    "            print(step, cost_val, acc_val)\n",
    "    # Accuracy report        \n",
    "    h = sess.run(hypothesis, feed_dict = {X : x_data, Y : y_data})\n",
    "    print(sess.run([tf.argmax(Y, 1),tf.argmax(h,1)], feed_dict={Y : y_data}))\n",
    "    \n",
    "    # Testing & One-hot encoding\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n",
    "    print(a, sess.run(tf.arg_max(a, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n",
    "    print(b, sess.run(tf.arg_max(b, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n",
    "    print(c, sess.run(tf.arg_max(c, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "\n",
    "    all = sess.run(hypothesis, feed_dict={\n",
    "                   X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n",
    "    print(all, sess.run(tf.arg_max(all, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : zoo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'data-01-test-score.csv',\n",
       " 'data-02-test-score.csv',\n",
       " 'data-03-test-score.csv',\n",
       " 'data-04-test-score.csv',\n",
       " 'data-05-test-score.csv',\n",
       " 'diabetes.csv',\n",
       " 'DLEL.zip',\n",
       " 'DLEL_1_TensorFlow_Basics.ipynb',\n",
       " 'DLEL_2_Linear_regression.ipynb',\n",
       " 'DLEL_3_Minimizing_Cost.ipynb',\n",
       " 'DLEL_4_2_Loading_Data_from_File.ipynb',\n",
       " 'DLEL_4_Multi-variable_linear_regression.ipynb',\n",
       " 'DLEL_5_Logistic_regression.ipynb',\n",
       " 'DLEL_6_Softmax_Classifier.ipynb',\n",
       " 'zoo.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 1)\n"
     ]
    }
   ],
   "source": [
    "# load zoo dataset\n",
    "xy = np.loadtxt(fname = './zoo.csv', delimiter = ',')\n",
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:,[-1]]\n",
    "nb_classes = 7\n",
    "print(x_data.shape, y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(?, 7), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(dtype = tf.float32, shape = [None,x_data.shape[1]])\n",
    "Y = tf.placeholder(dtype = tf.int32, shape = [None, 1])\n",
    "\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, shape = [-1,nb_classes])\n",
    "print(Y_one_hot)\n",
    "\n",
    "W = tf.Variable(initial_value = tf.random_normal(shape = [x_data.shape[1], nb_classes]), name = 'weight')\n",
    "b = tf.Variable(initial_value = tf.random_normal(shape = [nb_classes]), name = 'bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross entropy cost/loss\n",
    "logits = tf.matmul(X, W) + b  #pre-activation\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "tmp_cost = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y_one_hot)\n",
    "cost = tf.reduce_mean(tmp_cost)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
    "\n",
    "# prediction & accuracy\n",
    "prediction = tf.argmax(hypothesis, axis = 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_one_hot, axis = 1), prediction), dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step :     0, loss : 7.9944, acc : 22.77%\n",
      "step :   200, loss : 2.7889, acc : 27.72%\n",
      "step :   400, loss : 1.6963, acc : 47.52%\n",
      "step :   600, loss : 1.1950, acc : 57.43%\n",
      "step :   800, loss : 0.9330, acc : 73.27%\n",
      "step :  1000, loss : 0.7769, acc : 77.23%\n",
      "step :  1200, loss : 0.6701, acc : 78.22%\n",
      "step :  1400, loss : 0.5901, acc : 82.18%\n",
      "step :  1600, loss : 0.5268, acc : 84.16%\n",
      "step :  1800, loss : 0.4751, acc : 86.14%\n",
      "step :  2000, loss : 0.4317, acc : 89.11%\n",
      "[True] Prediction : 1.0 True Y : 1\n",
      "[True] Prediction : 0.0 True Y : 0\n",
      "[True] Prediction : 0.0 True Y : 0\n",
      "[True] Prediction : 0.0 True Y : 0\n",
      "[True] Prediction : 0.0 True Y : 0\n",
      "[False] Prediction : 6.0 True Y : 5\n",
      "[True] Prediction : 0.0 True Y : 0\n",
      "[True] Prediction : 0.0 True Y : 0\n",
      "[True] Prediction : 0.0 True Y : 0\n",
      "[True] Prediction : 0.0 True Y : 0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(2001):\n",
    "        sess.run(opt, feed_dict = {X : x_data, Y : y_data})\n",
    "        if step % 200 == 0:\n",
    "            loss, acc = sess.run(fetches = [cost, accuracy], feed_dict = {X : x_data, Y : y_data})\n",
    "            print('step : {:5}, loss : {:.4f}, acc : {:.2%}'.format(step, loss, acc))\n",
    "            \n",
    "    # Let's see if we can predict (임의의 10개를 뽑아서 예측 결과를 확인)\n",
    "    pred = sess.run(prediction, feed_dict = {X : x_data})\n",
    "    for p, y in np.asarray(list(zip(pred, y_data.flatten())))[list(np.random.randint(low = 0, high = y_data.shape[0], size = 10))]:\n",
    "        print('[{}] Prediction : {} True Y : {}'.format(p == int(y), p, int(y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
