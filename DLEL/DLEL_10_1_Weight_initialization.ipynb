{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모두를 위한 머신러닝/딥러닝 강의\n",
    "김성훈 교수님의 모두를 위한 머신러닝/딥러닝 강의 중 lab 강의 코드입니다.\n",
    "## Lab10_1 Weight initialization\n",
    "강의에서 다룬 Weight initialization 방법론들을 mnist data에 활용해보는 코드, 네트워크는 2개의 hidden layer를 가지는 (784, 512, 256, 10)의 구조\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization : normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\dev \n",
      " ['.ipynb_checkpoints', 'Data structures and algorithms_warm_up.ipynb', 'Data structures and algorithms_week_1.ipynb', 'DLEL', 'DLEL.zip', 'DLEL_10_1_Weight_initialization.ipynb', 'logs', 'MNIST_data', 'py-automate', 'Untitled.ipynb', 'untitled.txt', 'Untitled1.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(777) # for reproducibility\n",
    "print(os.getcwd(),'\\n', os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "## load data and parameter\n",
    "mnist = input_data.read_data_sets('./MNIST_data/', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# graph setting\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [None, 784], name = 'Xinput')\n",
    "Y = tf.placeholder(dtype = tf.float32, shape = [None, 10], name = 'Yinput')\n",
    "\n",
    "with tf.variable_scope('layer1'):\n",
    "    w1 = tf.Variable(initial_value = tf.random_normal(shape = [784, 512]), name = 'w1')\n",
    "    b1 = tf.Variable(initial_value = tf.random_normal(shape = [512]), name = 'b1')\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, w1) + b1)\n",
    "    layer1_hist = tf.summary.histogram(name = 'layer1', values = layer1)\n",
    "\n",
    "with tf.variable_scope('layer2'):\n",
    "    w2 = tf.Variable(initial_value = tf.random_normal(shape = [512, 256]), name = 'w2')\n",
    "    b2 = tf.Variable(initial_value = tf.random_normal(shape = [256]), name = 'b2')\n",
    "    layer2 = tf.sigmoid(tf.matmul(layer1, w2) + b2)\n",
    "    layer2_hist = tf.summary.histogram(name = 'layer2', values = layer2)\n",
    "\n",
    "with tf.variable_scope('layer3'):\n",
    "    w3 = tf.Variable(initial_value = tf.random_normal(shape = [256, 10]), name = 'w3')\n",
    "    b3 = tf.Variable(initial_value = tf.random_normal(shape = [10]), name = 'b3')\n",
    "    score = tf.matmul(layer2, w3) + b3\n",
    "    layer3 = tf.sigmoid(tf.matmul(layer2, w3) + b3)\n",
    "    layer3_hist = tf.summary.histogram(name = 'layer3', values = layer3)\n",
    "\n",
    "with tf.variable_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = score, labels = Y))\n",
    "    loss_scalar = tf.summary.scalar(name = 'loss', tensor = loss)\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(loss)\n",
    "    \n",
    "with tf.variable_scope('accuracy'):\n",
    "    is_correct = tf.equal(tf.argmax(score, 1), tf.argmax(Y, axis = 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype = tf.float32))\n",
    "    accuracy_scalar = tf.summary.scalar(name = 'accuracy', tensor = accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :    0, tr_loss : 3.565, val_loss : 3.189\n",
      "epoch :    5, tr_loss : 1.171, val_loss : 1.224\n",
      "epoch :   10, tr_loss : 1.070, val_loss : 0.932\n",
      "epoch :   15, tr_loss : 0.550, val_loss : 0.793\n",
      "epoch :   20, tr_loss : 0.718, val_loss : 0.708\n",
      "epoch :   25, tr_loss : 0.617, val_loss : 0.648\n",
      "epoch :   30, tr_loss : 0.388, val_loss : 0.605\n",
      "epoch :   35, tr_loss : 0.717, val_loss : 0.572\n",
      "epoch :   40, tr_loss : 0.562, val_loss : 0.546\n",
      "epoch :   45, tr_loss : 0.675, val_loss : 0.523\n",
      "epoch :   50, tr_loss : 0.332, val_loss : 0.505\n",
      "epoch :   55, tr_loss : 0.360, val_loss : 0.489\n",
      "epoch :   60, tr_loss : 0.236, val_loss : 0.476\n",
      "epoch :   65, tr_loss : 0.419, val_loss : 0.464\n",
      "epoch :   70, tr_loss : 0.234, val_loss : 0.453\n",
      "epoch :   75, tr_loss : 0.333, val_loss : 0.443\n",
      "epoch :   80, tr_loss : 0.142, val_loss : 0.436\n",
      "epoch :   85, tr_loss : 0.389, val_loss : 0.428\n",
      "epoch :   90, tr_loss : 0.216, val_loss : 0.420\n",
      "epoch :   95, tr_loss : 0.305, val_loss : 0.414\n",
      "epoch :   99, tr_loss : 0.177, val_loss : 0.409\n",
      "0.8914\n"
     ]
    }
   ],
   "source": [
    "# parameter \n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('./logs/normal')\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for step in range(total_batch):\n",
    "            \n",
    "            tr_batch_xs, tr_batch_ys = mnist.train.next_batch(batch_size = batch_size)\n",
    "            val_batch_xs, val_batch_ys = mnist.validation.next_batch(batch_size = mnist.validation.num_examples)\n",
    "            summary, _, loss_tr = sess.run([merged_summary, train, loss], feed_dict = {X : tr_batch_xs, Y : tr_batch_ys})\n",
    "            loss_val = sess.run(loss, feed_dict = {X : val_batch_xs, Y : val_batch_ys})\n",
    "                \n",
    "        if epoch % 5 == 0 or epoch == (epochs - 1):\n",
    "            print('epoch : {:4}, tr_loss : {:.3f}, val_loss : {:.3f}'.format(epoch, loss_tr, loss_val))\n",
    "        writer.add_summary(summary, global_step = epoch)\n",
    "    \n",
    "    print(sess.run(accuracy, feed_dict = {X : val_batch_xs, Y : val_batch_ys}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization : Xavier initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# graph setting\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [None, 784], name = 'Xinput')\n",
    "Y = tf.placeholder(dtype = tf.float32, shape = [None, 10], name = 'Yinput')\n",
    "\n",
    "with tf.variable_scope('layer1'):\n",
    "    w1 = tf.get_variable(name = 'w1', shape = [784, 512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(initial_value = tf.random_normal(shape = [512]), name = 'b1')\n",
    "    layer1 = tf.sigmoid(tf.matmul(X, w1) + b1)\n",
    "    layer1_hist = tf.summary.histogram(name = 'layer1', values = layer1)\n",
    "\n",
    "with tf.variable_scope('layer2'):\n",
    "    w2 = tf.get_variable(name = 'w2', shape = [512, 256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(initial_value = tf.random_normal(shape = [256]), name = 'b2')\n",
    "    layer2 = tf.sigmoid(tf.matmul(layer1, w2) + b2)\n",
    "    layer2_hist = tf.summary.histogram(name = 'layer2', values = layer2)\n",
    "\n",
    "with tf.variable_scope('layer3'):\n",
    "    w3 = tf.get_variable(name = 'w3', shape = [256, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(initial_value = tf.random_normal(shape = [10]), name = 'b3')\n",
    "    score = tf.matmul(layer2, w3) + b3\n",
    "    layer3 = tf.sigmoid(tf.matmul(layer2, w3) + b3)\n",
    "    layer3_hist = tf.summary.histogram(name = 'layer3', values = layer3)\n",
    "\n",
    "with tf.variable_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = score, labels = Y))\n",
    "    loss_scalar = tf.summary.scalar(name = 'loss', tensor = loss)\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(loss)\n",
    "    \n",
    "with tf.variable_scope('accuracy'):\n",
    "    is_correct = tf.equal(tf.argmax(score, 1), tf.argmax(Y, axis = 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype = tf.float32))\n",
    "    accuracy_scalar = tf.summary.scalar(name = 'accuracy', tensor = accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :    0, tr_loss : 2.265, val_loss : 2.245\n",
      "epoch :    5, tr_loss : 1.353, val_loss : 1.397\n",
      "epoch :   10, tr_loss : 0.567, val_loss : 0.686\n",
      "epoch :   15, tr_loss : 0.575, val_loss : 0.500\n",
      "epoch :   20, tr_loss : 0.439, val_loss : 0.422\n",
      "epoch :   25, tr_loss : 0.385, val_loss : 0.379\n",
      "epoch :   30, tr_loss : 0.372, val_loss : 0.353\n",
      "epoch :   35, tr_loss : 0.276, val_loss : 0.335\n",
      "epoch :   40, tr_loss : 0.332, val_loss : 0.321\n",
      "epoch :   45, tr_loss : 0.517, val_loss : 0.311\n",
      "epoch :   50, tr_loss : 0.366, val_loss : 0.302\n",
      "epoch :   55, tr_loss : 0.393, val_loss : 0.293\n",
      "epoch :   60, tr_loss : 0.308, val_loss : 0.288\n",
      "epoch :   65, tr_loss : 0.277, val_loss : 0.280\n",
      "epoch :   70, tr_loss : 0.283, val_loss : 0.276\n",
      "epoch :   75, tr_loss : 0.203, val_loss : 0.271\n",
      "epoch :   80, tr_loss : 0.264, val_loss : 0.267\n",
      "epoch :   85, tr_loss : 0.367, val_loss : 0.262\n",
      "epoch :   90, tr_loss : 0.402, val_loss : 0.260\n",
      "epoch :   95, tr_loss : 0.221, val_loss : 0.255\n",
      "epoch :   99, tr_loss : 0.221, val_loss : 0.252\n",
      "0.9294\n"
     ]
    }
   ],
   "source": [
    "# parameter \n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('./logs/xavier')\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for step in range(total_batch):\n",
    "            \n",
    "            tr_batch_xs, tr_batch_ys = mnist.train.next_batch(batch_size = batch_size)\n",
    "            val_batch_xs, val_batch_ys = mnist.validation.next_batch(batch_size = mnist.validation.num_examples)\n",
    "            summary, _, loss_tr = sess.run([merged_summary, train, loss], feed_dict = {X : tr_batch_xs, Y : tr_batch_ys})\n",
    "            loss_val = sess.run(loss, feed_dict = {X : val_batch_xs, Y : val_batch_ys})\n",
    "                \n",
    "        if epoch % 5 == 0 or epoch == (epochs - 1):\n",
    "            print('epoch : {:4}, tr_loss : {:.3f}, val_loss : {:.3f}'.format(epoch, loss_tr, loss_val))\n",
    "        writer.add_summary(summary, global_step = epoch)\n",
    "    \n",
    "    print(sess.run(accuracy, feed_dict = {X : val_batch_xs, Y : val_batch_ys}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization : He initialization (with Relu activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define He initializer\n",
    "def he_init(n_input, n_output):\n",
    "    stddev = tf.sqrt(2.0 / (n_input))\n",
    "    return tf.truncated_normal_initializer(stddev = stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# graph setting\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [None, 784], name = 'Xinput')\n",
    "Y = tf.placeholder(dtype = tf.float32, shape = [None, 10], name = 'Yinput')\n",
    "\n",
    "with tf.variable_scope('layer1'):\n",
    "    w1 = tf.get_variable(name = 'w1', shape = [784, 512], initializer = he_init(784, 512))\n",
    "    b1 = tf.Variable(initial_value = tf.random_normal(shape = [512]), name = 'b1')\n",
    "    layer1 = tf.nn.relu(tf.matmul(X, w1) + b1)\n",
    "    layer1_hist = tf.summary.histogram(name = 'layer1', values = layer1)\n",
    "\n",
    "with tf.variable_scope('layer2'):\n",
    "    w2 = tf.get_variable(name = 'w2', shape = [512, 256], initializer = he_init(512, 256))\n",
    "    b2 = tf.Variable(initial_value = tf.random_normal(shape = [256]), name = 'b2')\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, w2) + b2)\n",
    "    layer2_hist = tf.summary.histogram(name = 'layer2', values = layer2)\n",
    "\n",
    "with tf.variable_scope('layer3'):\n",
    "    w3 = tf.get_variable(name = 'w3', shape = [256, 10], initializer = he_init(256, 10))\n",
    "    b3 = tf.Variable(initial_value = tf.random_normal(shape = [10]), name = 'b3')\n",
    "    score = tf.matmul(layer2, w3) + b3\n",
    "    layer3 = tf.nn.relu(tf.matmul(layer2, w3) + b3)\n",
    "    layer3_hist = tf.summary.histogram(name = 'layer3', values = layer3)\n",
    "\n",
    "with tf.variable_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = score, labels = Y))\n",
    "    loss_scalar = tf.summary.scalar(name = 'loss', tensor = loss)\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(loss)\n",
    "    \n",
    "with tf.variable_scope('accuracy'):\n",
    "    is_correct = tf.equal(tf.argmax(score, 1), tf.argmax(Y, axis = 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype = tf.float32))\n",
    "    accuracy_scalar = tf.summary.scalar(name = 'accuracy', tensor = accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :    0, tr_loss : 0.397, val_loss : 0.490\n",
      "epoch :    5, tr_loss : 0.284, val_loss : 0.266\n",
      "epoch :   10, tr_loss : 0.268, val_loss : 0.217\n",
      "epoch :   15, tr_loss : 0.141, val_loss : 0.182\n",
      "epoch :   20, tr_loss : 0.136, val_loss : 0.154\n",
      "epoch :   25, tr_loss : 0.098, val_loss : 0.133\n",
      "epoch :   30, tr_loss : 0.086, val_loss : 0.119\n",
      "epoch :   35, tr_loss : 0.091, val_loss : 0.110\n",
      "epoch :   40, tr_loss : 0.030, val_loss : 0.101\n",
      "epoch :   45, tr_loss : 0.053, val_loss : 0.094\n",
      "epoch :   50, tr_loss : 0.103, val_loss : 0.089\n",
      "epoch :   55, tr_loss : 0.086, val_loss : 0.085\n",
      "epoch :   60, tr_loss : 0.068, val_loss : 0.082\n",
      "epoch :   65, tr_loss : 0.090, val_loss : 0.079\n",
      "epoch :   70, tr_loss : 0.023, val_loss : 0.078\n",
      "epoch :   75, tr_loss : 0.142, val_loss : 0.075\n",
      "epoch :   80, tr_loss : 0.030, val_loss : 0.075\n",
      "epoch :   85, tr_loss : 0.035, val_loss : 0.073\n",
      "epoch :   90, tr_loss : 0.021, val_loss : 0.073\n",
      "epoch :   95, tr_loss : 0.025, val_loss : 0.071\n",
      "epoch :   99, tr_loss : 0.014, val_loss : 0.071\n",
      "0.9802\n"
     ]
    }
   ],
   "source": [
    "# parameter \n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter('./logs/he')\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for step in range(total_batch):\n",
    "            \n",
    "            tr_batch_xs, tr_batch_ys = mnist.train.next_batch(batch_size = batch_size)\n",
    "            val_batch_xs, val_batch_ys = mnist.validation.next_batch(batch_size = mnist.validation.num_examples)\n",
    "            summary, _, loss_tr = sess.run([merged_summary, train, loss], feed_dict = {X : tr_batch_xs, Y : tr_batch_ys})\n",
    "            loss_val = sess.run(loss, feed_dict = {X : val_batch_xs, Y : val_batch_ys})\n",
    "                \n",
    "        if epoch % 5 == 0 or epoch == (epochs - 1):\n",
    "            print('epoch : {:4}, tr_loss : {:.3f}, val_loss : {:.3f}'.format(epoch, loss_tr, loss_val))\n",
    "        writer.add_summary(summary, global_step = epoch)\n",
    "    \n",
    "    print(sess.run(accuracy, feed_dict = {X : val_batch_xs, Y : val_batch_ys}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard : graph\n",
    "![Alt text](http://i.imgur.com/ySlvADi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard : accuracy & loss  \n",
    "주황색은 Relu activation function을 사용하고 He initialization를 사용한 경우로 학습이 가장 빨리 이루어짐을 확인이 가능하다. sigmoid activation function을 쓴 하늘색과 보라색의 경우 xavier initialization을 사용한 경우가 학습이 빨리 이루어진 것을 확인할 수 있다.\n",
    "![Alt text](http://i.imgur.com/U6rw0uy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard : accuracy & loss  \n",
    "주황색은 Relu activation을 쓰고 He initialization을 쓴 경우로 다른 두 경우와 직접적인 비교가 어렵다. 나머지 sigmoid activation function을 쓴 경우, xavier initialization을 쓴 경우 activation 값이 정규분포모양의 형태로 고루퍼져있어 neural net의 표현력이 좋고, vanishing gradient 문제를 해결할 수 있음을 확인할 수 있다.\n",
    "![Alt text](http://i.imgur.com/hbZTCy2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
